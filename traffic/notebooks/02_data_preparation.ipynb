{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üìä Enhanced Data Preparation Pipeline\n",
    "## Traffic Accident Analysis Project\n",
    "\n",
    "This notebook demonstrates the enhanced data preparation pipeline with comprehensive preprocessing capabilities including:\n",
    "\n",
    "- **Smart Missing Value Handling** - Multiple strategies with validation\n",
    "- **Advanced Outlier Treatment** - IQR, Z-score, and robust methods\n",
    "- **Intelligent Data Transformation** - Automatic skewness detection\n",
    "- **Domain-Specific Feature Engineering** - Traffic-specific features\n",
    "- **Smart Categorical Encoding** - Cardinality-aware encoding\n",
    "- **Intelligent Feature Scaling** - Excludes binary features\n",
    "- **Advanced Class Balancing** - Multiple resampling methods\n",
    "- **Comprehensive Data Validation** - Quality checks and metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## üîß Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup completed successfully!\n",
      "üìÅ Working directory: d:\\traffic\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Add the parent directory to sys.path so we can import from 'src'\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.data.traffic_data_prep_pipeline import TrafficDataPrep\n",
    "\n",
    "print(\"‚úÖ Setup completed successfully!\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## üì• Data Loading with Quality Assessment\n",
    "\n",
    "The enhanced pipeline provides comprehensive data loading with automatic quality assessment and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded successfully.\n",
      "üìä Dataset shape: 209,306 rows √ó 24 columns\n",
      "üìà Data Quality Summary:\n",
      "   Missing values: 0 (0.0%)\n",
      "   Duplicate rows: 31\n",
      "   Numeric features: 10\n",
      "   Categorical features: 14\n",
      "\n",
      "üìã Dataset Overview:\n",
      "   Shape: 209,306 rows √ó 24 columns\n",
      "   Memory usage: 214.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_date</th>\n",
       "      <th>traffic_control_device</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>lighting_condition</th>\n",
       "      <th>first_crash_type</th>\n",
       "      <th>trafficway_type</th>\n",
       "      <th>alignment</th>\n",
       "      <th>roadway_surface_cond</th>\n",
       "      <th>road_defect</th>\n",
       "      <th>crash_type</th>\n",
       "      <th>...</th>\n",
       "      <th>most_severe_injury</th>\n",
       "      <th>injuries_total</th>\n",
       "      <th>injuries_fatal</th>\n",
       "      <th>injuries_incapacitating</th>\n",
       "      <th>injuries_non_incapacitating</th>\n",
       "      <th>injuries_reported_not_evident</th>\n",
       "      <th>injuries_no_indication</th>\n",
       "      <th>crash_hour</th>\n",
       "      <th>crash_day_of_week</th>\n",
       "      <th>crash_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/29/2023 01:00:00 PM</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>TURNING</td>\n",
       "      <td>NOT DIVIDED</td>\n",
       "      <td>STRAIGHT AND LEVEL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NO INJURY / DRIVE AWAY</td>\n",
       "      <td>...</td>\n",
       "      <td>NO INDICATION OF INJURY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/13/2023 12:11:00 AM</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DARKNESS, LIGHTED ROAD</td>\n",
       "      <td>TURNING</td>\n",
       "      <td>FOUR WAY</td>\n",
       "      <td>STRAIGHT AND LEVEL</td>\n",
       "      <td>DRY</td>\n",
       "      <td>NO DEFECTS</td>\n",
       "      <td>NO INJURY / DRIVE AWAY</td>\n",
       "      <td>...</td>\n",
       "      <td>NO INDICATION OF INJURY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/09/2021 10:30:00 AM</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>REAR END</td>\n",
       "      <td>T-INTERSECTION</td>\n",
       "      <td>STRAIGHT AND LEVEL</td>\n",
       "      <td>DRY</td>\n",
       "      <td>NO DEFECTS</td>\n",
       "      <td>NO INJURY / DRIVE AWAY</td>\n",
       "      <td>...</td>\n",
       "      <td>NO INDICATION OF INJURY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/09/2023 07:55:00 PM</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>ANGLE</td>\n",
       "      <td>FOUR WAY</td>\n",
       "      <td>STRAIGHT AND LEVEL</td>\n",
       "      <td>DRY</td>\n",
       "      <td>NO DEFECTS</td>\n",
       "      <td>INJURY AND / OR TOW DUE TO CRASH</td>\n",
       "      <td>...</td>\n",
       "      <td>NONINCAPACITATING INJURY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08/19/2023 02:55:00 PM</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>REAR END</td>\n",
       "      <td>T-INTERSECTION</td>\n",
       "      <td>STRAIGHT AND LEVEL</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NO INJURY / DRIVE AWAY</td>\n",
       "      <td>...</td>\n",
       "      <td>NO INDICATION OF INJURY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               crash_date traffic_control_device weather_condition  \\\n",
       "0  07/29/2023 01:00:00 PM         TRAFFIC SIGNAL             CLEAR   \n",
       "1  08/13/2023 12:11:00 AM         TRAFFIC SIGNAL             CLEAR   \n",
       "2  12/09/2021 10:30:00 AM         TRAFFIC SIGNAL             CLEAR   \n",
       "3  08/09/2023 07:55:00 PM         TRAFFIC SIGNAL             CLEAR   \n",
       "4  08/19/2023 02:55:00 PM         TRAFFIC SIGNAL             CLEAR   \n",
       "\n",
       "       lighting_condition first_crash_type trafficway_type  \\\n",
       "0                DAYLIGHT          TURNING     NOT DIVIDED   \n",
       "1  DARKNESS, LIGHTED ROAD          TURNING        FOUR WAY   \n",
       "2                DAYLIGHT         REAR END  T-INTERSECTION   \n",
       "3                DAYLIGHT            ANGLE        FOUR WAY   \n",
       "4                DAYLIGHT         REAR END  T-INTERSECTION   \n",
       "\n",
       "            alignment roadway_surface_cond road_defect  \\\n",
       "0  STRAIGHT AND LEVEL              UNKNOWN     UNKNOWN   \n",
       "1  STRAIGHT AND LEVEL                  DRY  NO DEFECTS   \n",
       "2  STRAIGHT AND LEVEL                  DRY  NO DEFECTS   \n",
       "3  STRAIGHT AND LEVEL                  DRY  NO DEFECTS   \n",
       "4  STRAIGHT AND LEVEL              UNKNOWN     UNKNOWN   \n",
       "\n",
       "                         crash_type  ...        most_severe_injury  \\\n",
       "0            NO INJURY / DRIVE AWAY  ...   NO INDICATION OF INJURY   \n",
       "1            NO INJURY / DRIVE AWAY  ...   NO INDICATION OF INJURY   \n",
       "2            NO INJURY / DRIVE AWAY  ...   NO INDICATION OF INJURY   \n",
       "3  INJURY AND / OR TOW DUE TO CRASH  ...  NONINCAPACITATING INJURY   \n",
       "4            NO INJURY / DRIVE AWAY  ...   NO INDICATION OF INJURY   \n",
       "\n",
       "  injuries_total injuries_fatal  injuries_incapacitating  \\\n",
       "0            0.0            0.0                      0.0   \n",
       "1            0.0            0.0                      0.0   \n",
       "2            0.0            0.0                      0.0   \n",
       "3            5.0            0.0                      0.0   \n",
       "4            0.0            0.0                      0.0   \n",
       "\n",
       "  injuries_non_incapacitating  injuries_reported_not_evident  \\\n",
       "0                         0.0                            0.0   \n",
       "1                         0.0                            0.0   \n",
       "2                         0.0                            0.0   \n",
       "3                         5.0                            0.0   \n",
       "4                         0.0                            0.0   \n",
       "\n",
       "   injuries_no_indication  crash_hour  crash_day_of_week  crash_month  \n",
       "0                     3.0          13                  7            7  \n",
       "1                     2.0           0                  1            8  \n",
       "2                     3.0          10                  5           12  \n",
       "3                     0.0          19                  4            8  \n",
       "4                     3.0          14                  7            8  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the enhanced data preparation pipeline\n",
    "prep = TrafficDataPrep(\"../data/raw/traffic_accidents.csv\")\n",
    "\n",
    "# Load data with automatic quality assessment\n",
    "df = prep.load_data()\n",
    "\n",
    "print(f\"\\nüìã Dataset Overview:\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing_values",
   "metadata": {},
   "source": [
    "## üîç Enhanced Missing Value Handling\n",
    "\n",
    "Smart missing value imputation with multiple strategies:\n",
    "- **Categorical features**: Mode imputation\n",
    "- **Numeric features**: Median imputation (robust to outliers)\n",
    "- **High missing percentage**: Automatic detection and warnings\n",
    "- **Comprehensive visualization**: Before/after comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handle_missing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç MISSING VALUE HANDLING\n",
      "============================================================\n",
      "‚úÖ No missing values found!\n",
      "\n",
      "üìä Missing Value Summary:\n",
      "   Total missing values after cleaning: 0\n",
      "   Data completeness: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values with enhanced strategy\n",
    "df = prep.handle_missing_values(strategy='auto')\n",
    "\n",
    "print(f\"\\nüìä Missing Value Summary:\")\n",
    "missing_after = df.isnull().sum().sum()\n",
    "print(f\"   Total missing values after cleaning: {missing_after:,}\")\n",
    "print(f\"   Data completeness: {((df.size - missing_after) / df.size * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outlier_treatment",
   "metadata": {},
   "source": [
    "## üì¶ Advanced Outlier Treatment\n",
    "\n",
    "Multiple outlier detection and treatment methods:\n",
    "- **IQR Method**: Interquartile range-based detection\n",
    "- **Z-Score Method**: Standard deviation-based detection\n",
    "- **Robust Method**: Median absolute deviation-based\n",
    "- **Capping Strategy**: Preserves data while removing extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outlier_treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üì¶ OUTLIER TREATMENT\n",
      "============================================================\n",
      "üîç Analyzing outliers in 4 features using IQR method\n",
      "\n",
      "üìä Processing 'injuries_total':\n",
      "   Original range: 0.00 to 21.00\n",
      "   Outliers detected: 5,692 (2.7%)\n",
      "   After capping: 0.00 to 2.50\n",
      "\n",
      "üìä Processing 'injuries_incapacitating':\n",
      "   Original range: 0.00 to 7.00\n",
      "   Outliers detected: 6,634 (3.2%)\n",
      "   After capping: 0.00 to 0.00\n",
      "\n",
      "üìä Processing 'injuries_non_incapacitating':\n",
      "   Original range: 0.00 to 21.00\n",
      "   Outliers detected: 33,000 (15.8%)\n",
      "   After capping: 0.00 to 0.00\n",
      "\n",
      "üìä Processing 'num_units':\n",
      "   Original range: 1.00 to 11.00\n",
      "   Outliers detected: 19,940 (9.5%)\n",
      "   After capping: 2.00 to 2.00\n"
     ]
    }
   ],
   "source": [
    "# Apply advanced outlier treatment\n",
    "df = prep.outlier_treatment(method='iqr', threshold=1.5)\n",
    "\n",
    "print(f\"\\nüìà Outlier Treatment Results:\")\n",
    "print(f\"   Method used: IQR with threshold 1.5\")\n",
    "print(f\"   Strategy: Capping (preserves data size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_transformation",
   "metadata": {},
   "source": [
    "## üîÑ Intelligent Data Transformation\n",
    "\n",
    "Automatic skewness detection and transformation:\n",
    "- **Skewness Analysis**: Automatic detection of skewed features\n",
    "- **Log Transformation**: Applied to highly skewed features\n",
    "- **Threshold-Based**: Configurable skewness threshold\n",
    "- **Before/After Visualization**: Distribution comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply intelligent data transformation\n",
    "df = prep.data_transformation(auto_detect_skew=True, skew_threshold=1.0)\n",
    "\n",
    "print(f\"\\nüîÑ Transformation Summary:\")\n",
    "print(f\"   Auto-detection enabled with threshold: 1.0\")\n",
    "print(f\"   Method: Log1p transformation (handles zeros)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_engineering",
   "metadata": {},
   "source": [
    "## üîß Domain-Specific Feature Engineering\n",
    "\n",
    "Traffic accident domain knowledge applied to create meaningful features:\n",
    "- **Time-Based Features**: Night indicator, rush hours, time periods\n",
    "- **Risk Assessment**: Composite risk scores\n",
    "- **Injury Analysis**: Severe injury indicators, injury rates\n",
    "- **Interaction Features**: Combined risk factors\n",
    "- **Weekend/Weekday**: Day-based patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create domain-specific features\n",
    "df = prep.feature_engineering(create_interactions=True)\n",
    "\n",
    "print(f\"\\nüîß Feature Engineering Results:\")\n",
    "print(f\"   New dataset shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Features added: Time-based, risk assessment, injury analysis\")\n",
    "\n",
    "# Show new feature columns\n",
    "new_features = [col for col in df.columns if any(keyword in col.lower() \n",
    "                for keyword in ['night', 'rush', 'weekend', 'risk', 'severe'])]\n",
    "if new_features:\n",
    "    print(f\"\\nüÜï New Features Created: {', '.join(new_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_encoding",
   "metadata": {},
   "source": [
    "## üî§ Smart Categorical Encoding\n",
    "\n",
    "Cardinality-aware encoding strategy:\n",
    "- **Auto Strategy**: Chooses encoding based on cardinality\n",
    "- **Label Encoding**: For ordinal and high-cardinality features\n",
    "- **Cardinality Warnings**: Alerts for high-cardinality features\n",
    "- **Encoder Storage**: Saves encoders for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply smart categorical encoding\n",
    "df = prep.encode_features(encoding_strategy='auto')\n",
    "\n",
    "print(f\"\\nüî§ Encoding Results:\")\n",
    "print(f\"   Categorical features processed: {len(prep.encoders)}\")\n",
    "print(f\"   Encoding strategy: Auto (cardinality-aware)\")\n",
    "\n",
    "# Show data types after encoding\n",
    "print(f\"\\nüìä Data Types After Encoding:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   {dtype}: {count} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_scaling",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Intelligent Feature Scaling\n",
    "\n",
    "Smart scaling that preserves binary features:\n",
    "- **Binary Detection**: Automatically identifies binary features\n",
    "- **Selective Scaling**: Excludes binary features from scaling\n",
    "- **Multiple Methods**: Standard, Robust, MinMax scaling\n",
    "- **Before/After Comparison**: Statistical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_scaling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply intelligent feature scaling\n",
    "df = prep.scale_features(scaling_method='standard', exclude_binary=True)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Scaling Results:\")\n",
    "print(f\"   Method: Standard scaling\")\n",
    "print(f\"   Binary features excluded: Yes\")\n",
    "print(f\"   Scaler stored for future use: Yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class_balancing",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Advanced Class Imbalance Handling\n",
    "\n",
    "Multiple resampling techniques for class imbalance:\n",
    "- **SMOTE**: Synthetic Minority Oversampling Technique\n",
    "- **ADASYN**: Adaptive Synthetic Sampling\n",
    "- **BorderlineSMOTE**: Borderline cases focus\n",
    "- **Imbalance Analysis**: Automatic ratio calculation\n",
    "- **Before/After Visualization**: Class distribution comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "class_balancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance with advanced methods\n",
    "df = prep.handle_imbalance(method='smote', sampling_strategy='auto')\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Class Balancing Results:\")\n",
    "print(f\"   Method: SMOTE (Synthetic Minority Oversampling)\")\n",
    "print(f\"   Final dataset shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "# Show final class distribution\n",
    "target_dist = df[prep.target].value_counts().sort_index()\n",
    "print(f\"\\nüéØ Final Class Distribution:\")\n",
    "for class_val, count in target_dist.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"   Class {class_val}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_validation",
   "metadata": {},
   "source": [
    "## ‚úÖ Comprehensive Data Quality Validation\n",
    "\n",
    "Final data quality checks and validation:\n",
    "- **Missing Values Check**: Ensures no missing data\n",
    "- **Infinite Values Check**: Detects mathematical issues\n",
    "- **Data Types Validation**: Confirms proper types\n",
    "- **Target Variable Check**: Validates target classes\n",
    "- **Duplicate Detection**: Identifies duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive data quality validation\n",
    "validation_results = prep.validate_data_quality()\n",
    "\n",
    "print(f\"\\nüìã Validation Summary:\")\n",
    "passed = sum(1 for r in validation_results if r['status'] == 'PASS')\n",
    "warnings = sum(1 for r in validation_results if r['status'] == 'WARN')\n",
    "failed = sum(1 for r in validation_results if r['status'] == 'FAIL')\n",
    "\n",
    "print(f\"   ‚úÖ Passed: {passed}\")\n",
    "print(f\"   ‚ö†Ô∏è Warnings: {warnings}\")\n",
    "print(f\"   ‚ùå Failed: {failed}\")\n",
    "\n",
    "if failed == 0:\n",
    "    print(f\"\\nüéâ Data is ready for modeling!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Please review failed checks before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preparation_summary",
   "metadata": {},
   "source": [
    "## üìã Complete Preparation Summary\n",
    "\n",
    "Comprehensive summary of all preprocessing steps and transformations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prep_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive preparation summary\n",
    "summary = prep.generate_preparation_summary()\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Characteristics:\")\n",
    "print(f\"   Total samples: {df.shape[0]:,}\")\n",
    "print(f\"   Total features: {df.shape[1]}\")\n",
    "print(f\"   Numeric features: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"   Target variable: {prep.target}\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_data",
   "metadata": {},
   "source": [
    "## üíæ Save Model-Ready Data\n",
    "\n",
    "Save the fully processed dataset with comprehensive metadata for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_processed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model-ready dataset with metadata\n",
    "output_path = \"../data/processed/traffic_model_ready.csv\"\n",
    "prep.save_model_ready_data(output_path)\n",
    "\n",
    "print(f\"\\nüíæ Data Saved Successfully!\")\n",
    "print(f\"   Main dataset: {output_path}\")\n",
    "print(f\"   Metadata: {output_path.replace('.csv', '_metadata.json')}\")\n",
    "print(f\"   File size: {os.path.getsize(output_path) / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_test_split",
   "metadata": {},
   "source": [
    "## üîÑ Enhanced Train-Test-Validation Splits\n",
    "\n",
    "Create stratified splits with optional validation set:\n",
    "- **Stratified Sampling**: Maintains class distribution\n",
    "- **Three-Way Split**: Train, Test, and Validation sets\n",
    "- **Configurable Ratios**: Flexible split proportions\n",
    "- **Metadata Tracking**: Complete split information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced train-test-validation splits\n",
    "splits_info = prep.save_train_test_splits(\n",
    "    output_dir=\"../data/processed\",\n",
    "    test_size=0.2,\n",
    "    validation_size=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nüîÑ Split Creation Complete!\")\n",
    "print(f\"   Files created in: ../data/processed/\")\n",
    "print(f\"   Split configuration: 70% train, 20% test, 10% validation\")\n",
    "print(f\"   Stratified: Yes (maintains class distribution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_overview",
   "metadata": {},
   "source": [
    "## üéØ Final Data Overview\n",
    "\n",
    "Complete overview of the processed dataset ready for machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
